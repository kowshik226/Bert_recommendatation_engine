{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_Search_Engine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF8d7aK00U9l"
      },
      "source": [
        "**Semantic based recommendation engine with BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZI2KHfUNW8a"
      },
      "source": [
        "Below code loads the pre-trained SentenceTransformer model 'bert-base-nli-mean-tokens' from the server.\n",
        "It then fine-tunes this model for some epochs on the our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQAdQoMlQ9p7"
      },
      "source": [
        "# code adapted from https://github.com/UKPLab/sentence-transformers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCAgCaDryxpM"
      },
      "source": [
        "! pip install sentence_transformers\n",
        "! pip install torch\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "#Check if dataset exsist. If not, download and extract  it\n",
        "sts_dataset_path = ''\n",
        "\n",
        "# Read the dataset\n",
        "model_name = 'bert-base-nli-mean-tokens'\n",
        "train_batch_size = 16\n",
        "num_epochs = 4\n",
        "model_save_path = ''\n",
        "\n",
        "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
        "word_embedding_model = models.Transformer(model_name)\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "# Convert the dataset to a DataLoader ready for training\n",
        "# from sts_dataset_path tarin, dev and test samples will be divided\n",
        "train_samples = []\n",
        "dev_samples = []\n",
        "test_samples = []\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "\n",
        "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples)\n",
        "\n",
        "\n",
        "# Configure the training. We skip evaluation in this example\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1) #10% of train data for warm-up\n",
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          optimizer=\"adam\",\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)\n",
        "\n",
        "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
        "\n",
        "model = SentenceTransformer(model_save_path)\n",
        "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples)\n",
        "test_evaluator(model, output_path=model_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RotXgsYPRBbK"
      },
      "source": [
        "Below code  to load trained model and generate sentence embedding for our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsSdlV1iRBB5"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and \n",
        "# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\n",
        "\n",
        "model = SentenceTransformer(model_save_path)\n",
        "\n",
        "sentences = \"our dataset\"\n",
        "# Each sentence is encoded as a 1-D vector\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
        "\n",
        "print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEaM_0xaSN58"
      },
      "source": [
        "below code that takes search query, compute query embeddings and then computes distance between query embedding with each embeddings of each data in the corpus and arranges in asceding order of cosine similarity score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO_OV-LZSFIJ"
      },
      "source": [
        "query = \"\"\n",
        "\n",
        "\n",
        "queries = [query]\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "number_top_matches = 5 \n",
        "\n",
        "print(\"Semantic Search Results\")\n",
        "\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "    print(type(results))\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for idx, distance in results[0:number_top_matches]:\n",
        "        print(sentences[idx].strip(), \"(Cosine Score: %.4f)\" % (1-distance))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}